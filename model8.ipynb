{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T02:40:20.227463Z",
     "start_time": "2024-12-05T02:40:20.220490Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:40:20.242275Z",
     "start_time": "2024-12-05T02:40:20.231451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpotifyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SpotifyRankPredictor(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super(SpotifyRankPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, num_categories)  # 출력층이 1개에서 5개(카테고리 수)로 변경\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax 활성화 함수 추가하여 각 카테고리의 확률 출력\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.softmax(x)"
   ],
   "id": "bfeb3bf9ec46e9be",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:40:20.272702Z",
     "start_time": "2024-12-05T02:40:20.259243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_features(features, num_samples_needed):\n",
    "    augmented_data = []\n",
    "    feature_names = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                     'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']\n",
    "\n",
    "    noise_ranges = {\n",
    "        'Danceability': 0.05,\n",
    "        'Energy': 0.05,\n",
    "        'Loudness': 1.0,\n",
    "        'Speechiness': 0.02,\n",
    "        'Acousticness': 0.05,\n",
    "        'Liveness': 0.05,\n",
    "        'Tempo': 3.0,\n",
    "        'Duration (ms)': 0.05\n",
    "    }\n",
    "\n",
    "    for _ in range(num_samples_needed):\n",
    "        base_sample = features[np.random.randint(len(features))]\n",
    "        new_features = []\n",
    "\n",
    "        for feat_idx, feat_name in enumerate(feature_names):\n",
    "            feature = np.abs(base_sample[feat_idx])\n",
    "\n",
    "            if feat_name == 'Loudness':\n",
    "                noise = np.random.normal(0, np.abs(noise_ranges['Loudness']))\n",
    "            elif feat_name == 'Tempo':\n",
    "                noise = np.random.normal(0, np.abs(noise_ranges['Tempo']))\n",
    "            elif feat_name == 'Duration (ms)':\n",
    "                noise = np.random.normal(0, np.abs(feature * noise_ranges['Duration (ms)']))\n",
    "            else:\n",
    "                noise = np.random.normal(0, np.abs(noise_ranges[feat_name]))\n",
    "\n",
    "            new_value = feature + noise\n",
    "\n",
    "            if feat_name == 'Loudness':\n",
    "                new_value = np.clip(new_value, -60, 0)\n",
    "            elif feat_name == 'Duration (ms)':\n",
    "                new_value = max(1000, new_value)\n",
    "            else:\n",
    "                new_value = np.clip(new_value, 0, 1)\n",
    "\n",
    "            new_features.append(new_value)\n",
    "\n",
    "        augmented_data.append(new_features)\n",
    "\n",
    "    return np.array(augmented_data)"
   ],
   "id": "2aa7dd05d3541c3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:40:20.302605Z",
     "start_time": "2024-12-05T02:40:20.288653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(df):\n",
    "    X = df[['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "            'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']].values\n",
    "\n",
    "    # 특성 스케일링\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    def rank_to_category(rank):\n",
    "        if rank <= 10:\n",
    "            return 0\n",
    "        elif rank <= 30:\n",
    "            return 1\n",
    "        elif rank <= 50:\n",
    "            return 2\n",
    "        elif rank <= 100:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    ranks = df['Highest Charting Position'].values\n",
    "    categories = np.array([rank_to_category(rank) for rank in ranks])\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    num_categories = 5\n",
    "    y_encoded = np.eye(num_categories)[categories]\n",
    "\n",
    "    return X_scaled, y_encoded, num_categories\n",
    "\n",
    "def balance_dataset(X, y):\n",
    "    # one-hot 인코딩된 y를 다시 카테고리로 변환\n",
    "    categories = np.argmax(y, axis=1)\n",
    "\n",
    "    # 각 카테고리별 샘플 수 계산\n",
    "    category_counts = np.bincount(categories)\n",
    "    print(\"Original category counts in train set:\", category_counts)\n",
    "\n",
    "    # 목표 샘플 수 설정 (Below 100 클래스 샘플 수의 80%)\n",
    "    target_samples = int(category_counts[4] * 0.8)\n",
    "    print(\"Target samples per category:\", target_samples)\n",
    "\n",
    "    processed_features = []\n",
    "    processed_categories = []\n",
    "\n",
    "    for category in range(5):\n",
    "        category_mask = categories == category\n",
    "        category_features = X[category_mask]\n",
    "        current_samples = len(category_features)\n",
    "\n",
    "        if current_samples < target_samples:\n",
    "            # 증강이 필요한 경우\n",
    "            num_samples_needed = target_samples - current_samples\n",
    "            augmented = augment_features(category_features, num_samples_needed)\n",
    "            processed_features.append(np.vstack([category_features, augmented]))\n",
    "            processed_categories.extend([category] * target_samples)\n",
    "\n",
    "        elif current_samples > target_samples:\n",
    "            # Under-sampling이 필요한 경우\n",
    "            indices = np.random.choice(current_samples, target_samples, replace=False)\n",
    "            processed_features.append(category_features[indices])\n",
    "            processed_categories.extend([category] * target_samples)\n",
    "\n",
    "        else:\n",
    "            # 현재 샘플 수가 목표와 동일한 경우\n",
    "            processed_features.append(category_features)\n",
    "            processed_categories.extend([category] * current_samples)\n",
    "\n",
    "    # 모든 처리된 데이터 합치기\n",
    "    X_combined = np.vstack(processed_features)\n",
    "    categories_combined = np.array(processed_categories)\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    y_combined = np.eye(5)[categories_combined]\n",
    "\n",
    "    print(\"Final category counts in balanced train set:\", np.bincount(categories_combined))\n",
    "\n",
    "    return X_combined, y_combined"
   ],
   "id": "ec818e3d214c56b5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:40:20.332414Z",
     "start_time": "2024-12-05T02:40:20.318599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                num_epochs=100, patience=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)  # Classification 문제로 바뀌었기 때문에 이 부분 수정. 기존에는 y_batch(-1,1)여서 연속적인 값 예측시 사용하는 것\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, actual = torch.max(y_batch.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "id": "a481aa6b9ffbe221",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:40:23.308445Z",
     "start_time": "2024-12-05T02:40:20.350059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # 1. 데이터 로드 및 기본 전처리\n",
    "    df = pd.read_csv('spotify_dataset.csv')\n",
    "    X_scaled, y_encoded, num_categories = preprocess_data(df)\n",
    "\n",
    "    # 2. 데이터 분할 (증강 전)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3. train 데이터에만 balancing 적용\n",
    "    X_train_balanced, y_train_balanced = balance_dataset(X_train, y_train)\n",
    "\n",
    "    # 4. 데이터셋 생성\n",
    "    train_dataset = SpotifyDataset(X_train_balanced, y_train_balanced)\n",
    "    val_dataset = SpotifyDataset(X_val, y_val)\n",
    "    test_dataset = SpotifyDataset(X_test, y_test)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SpotifyRankPredictor(num_categories)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Test set 평가\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            y_true.extend(actual.numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "523e2a81871cdeda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original category counts in train set: [215 265 189 454 629]\n",
      "Target samples per category: 503\n",
      "Final category counts in balanced train set: [503 503 503 503 503]\n",
      "Epoch [1/100], Train Loss: 1.5745, Val Loss: 1.5372, Val Accuracy: 38.81%\n",
      "Epoch [2/100], Train Loss: 1.5506, Val Loss: 1.5237, Val Accuracy: 38.81%\n",
      "Epoch [3/100], Train Loss: 1.5460, Val Loss: 1.5176, Val Accuracy: 39.27%\n",
      "Epoch [4/100], Train Loss: 1.5416, Val Loss: 1.5122, Val Accuracy: 39.27%\n",
      "Epoch [5/100], Train Loss: 1.5397, Val Loss: 1.5070, Val Accuracy: 42.01%\n",
      "Epoch [6/100], Train Loss: 1.5367, Val Loss: 1.5027, Val Accuracy: 42.47%\n",
      "Epoch [7/100], Train Loss: 1.5346, Val Loss: 1.4978, Val Accuracy: 42.01%\n",
      "Epoch [8/100], Train Loss: 1.5328, Val Loss: 1.4975, Val Accuracy: 41.55%\n",
      "Epoch [9/100], Train Loss: 1.5343, Val Loss: 1.4975, Val Accuracy: 40.64%\n",
      "Epoch [10/100], Train Loss: 1.5336, Val Loss: 1.4952, Val Accuracy: 41.10%\n",
      "Epoch [11/100], Train Loss: 1.5324, Val Loss: 1.4923, Val Accuracy: 41.55%\n",
      "Epoch [12/100], Train Loss: 1.5300, Val Loss: 1.4908, Val Accuracy: 41.10%\n",
      "Epoch [13/100], Train Loss: 1.5325, Val Loss: 1.4926, Val Accuracy: 41.55%\n",
      "Epoch [14/100], Train Loss: 1.5332, Val Loss: 1.4914, Val Accuracy: 42.47%\n",
      "Epoch [15/100], Train Loss: 1.5295, Val Loss: 1.4924, Val Accuracy: 40.64%\n",
      "Epoch [16/100], Train Loss: 1.5286, Val Loss: 1.4926, Val Accuracy: 42.47%\n",
      "Epoch [17/100], Train Loss: 1.5302, Val Loss: 1.4957, Val Accuracy: 41.55%\n",
      "Epoch [18/100], Train Loss: 1.5293, Val Loss: 1.4975, Val Accuracy: 41.55%\n",
      "Epoch [19/100], Train Loss: 1.5316, Val Loss: 1.4911, Val Accuracy: 41.55%\n",
      "Epoch [20/100], Train Loss: 1.5311, Val Loss: 1.4912, Val Accuracy: 42.47%\n",
      "Epoch [21/100], Train Loss: 1.5295, Val Loss: 1.4924, Val Accuracy: 42.92%\n",
      "Epoch [22/100], Train Loss: 1.5280, Val Loss: 1.4904, Val Accuracy: 43.84%\n",
      "Epoch [23/100], Train Loss: 1.5285, Val Loss: 1.4941, Val Accuracy: 43.38%\n",
      "Epoch [24/100], Train Loss: 1.5296, Val Loss: 1.4855, Val Accuracy: 43.38%\n",
      "Epoch [25/100], Train Loss: 1.5292, Val Loss: 1.4913, Val Accuracy: 39.27%\n",
      "Epoch [26/100], Train Loss: 1.5299, Val Loss: 1.4955, Val Accuracy: 38.81%\n",
      "Epoch [27/100], Train Loss: 1.5284, Val Loss: 1.4894, Val Accuracy: 40.64%\n",
      "Epoch [28/100], Train Loss: 1.5277, Val Loss: 1.4950, Val Accuracy: 36.53%\n",
      "Epoch [29/100], Train Loss: 1.5268, Val Loss: 1.4926, Val Accuracy: 39.73%\n",
      "Epoch [30/100], Train Loss: 1.5261, Val Loss: 1.4883, Val Accuracy: 42.01%\n",
      "Epoch [31/100], Train Loss: 1.5274, Val Loss: 1.4899, Val Accuracy: 40.18%\n",
      "Epoch [32/100], Train Loss: 1.5287, Val Loss: 1.4859, Val Accuracy: 41.55%\n",
      "Epoch [33/100], Train Loss: 1.5261, Val Loss: 1.4912, Val Accuracy: 39.27%\n",
      "Epoch [34/100], Train Loss: 1.5271, Val Loss: 1.4977, Val Accuracy: 41.10%\n",
      "Early stopping triggered\n",
      "Test Accuracy: 40.18%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0  2 19]\n",
      " [ 0  0  0  5 26]\n",
      " [ 0  0  0  7 20]\n",
      " [ 0  0  0  9 42]\n",
      " [ 0  0  0 10 79]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이강민\\AppData\\Local\\Temp\\ipykernel_9104\\355451612.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
