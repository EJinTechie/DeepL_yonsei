{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-26T06:56:19.077203Z",
     "start_time": "2024-11-26T06:56:19.070226Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:56:19.092152Z",
     "start_time": "2024-11-26T06:56:19.082186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpotifyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SpotifyRankPredictor(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super(SpotifyRankPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, num_categories)  # 출력층이 1개에서 5개(카테고리 수)로 변경\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax 활성화 함수 추가하여 각 카테고리의 확률 출력\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.softmax(x)"
   ],
   "id": "bfeb3bf9ec46e9be",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:56:19.122077Z",
     "start_time": "2024-11-26T06:56:19.109121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_features(features, num_samples_needed):\n",
    "    augmented_data = []\n",
    "    feature_names = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                     'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']\n",
    "\n",
    "    noise_ranges = {\n",
    "        'Danceability': 0.05,\n",
    "        'Energy': 0.05,\n",
    "        'Loudness': 1.0,\n",
    "        'Speechiness': 0.02,\n",
    "        'Acousticness': 0.05,\n",
    "        'Liveness': 0.05,\n",
    "        'Tempo': 3.0,\n",
    "        'Duration (ms)': 0.05\n",
    "    }\n",
    "\n",
    "    for _ in range(num_samples_needed):\n",
    "        base_sample = features[np.random.randint(len(features))]\n",
    "        new_features = []\n",
    "\n",
    "        for feat_idx, feat_name in enumerate(feature_names):\n",
    "            feature = np.abs(base_sample[feat_idx])\n",
    "\n",
    "            if feat_name == 'Loudness':\n",
    "                noise = np.random.normal(0, np.abs(noise_ranges['Loudness']))\n",
    "            elif feat_name == 'Tempo':\n",
    "                noise = np.random.normal(0, np.abs(noise_ranges['Tempo']))\n",
    "            elif feat_name == 'Duration (ms)':\n",
    "                noise = np.random.normal(0, np.abs(feature * noise_ranges['Duration (ms)']))\n",
    "            else:\n",
    "                noise = np.random.normal(0, np.abs(noise_ranges[feat_name]))\n",
    "\n",
    "            new_value = feature + noise\n",
    "\n",
    "            if feat_name == 'Loudness':\n",
    "                new_value = np.clip(new_value, -60, 0)\n",
    "            elif feat_name == 'Duration (ms)':\n",
    "                new_value = max(1000, new_value)\n",
    "            else:\n",
    "                new_value = np.clip(new_value, 0, 1)\n",
    "\n",
    "            new_features.append(new_value)\n",
    "\n",
    "        augmented_data.append(new_features)\n",
    "\n",
    "    return np.array(augmented_data)"
   ],
   "id": "2aa7dd05d3541c3",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:56:19.151977Z",
     "start_time": "2024-11-26T06:56:19.138026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data_with_augmentation(df):\n",
    "    X = df[['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "            'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']].values\n",
    "\n",
    "    def rank_to_category(rank):\n",
    "        if rank <= 10:\n",
    "            return 0\n",
    "        elif rank <= 30:\n",
    "            return 1\n",
    "        elif rank <= 50:\n",
    "            return 2\n",
    "        elif rank <= 100:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    ranks = df['Highest Charting Position'].values\n",
    "    categories = np.array([rank_to_category(rank) for rank in ranks])\n",
    "\n",
    "    # 각 카테고리별 샘플 수 계산\n",
    "    category_counts = np.bincount(categories)\n",
    "    print(\"Original category counts:\", category_counts)\n",
    "\n",
    "    # 목표 샘플 수 설정 (Below 100 클래스 샘플 수의 80%)\n",
    "    target_samples = int(category_counts[4] * 0.8)\n",
    "    print(\"Target samples per category:\", target_samples)\n",
    "\n",
    "    processed_features = []\n",
    "    processed_categories = []\n",
    "\n",
    "    for category in range(5):\n",
    "        category_mask = categories == category\n",
    "        category_features = X[category_mask]\n",
    "        current_samples = len(category_features)\n",
    "\n",
    "        if current_samples < target_samples:\n",
    "            # 증강이 필요한 경우\n",
    "            num_samples_needed = target_samples - current_samples\n",
    "            augmented = augment_features(category_features, num_samples_needed)\n",
    "            processed_features.append(np.vstack([category_features, augmented]))\n",
    "            processed_categories.extend([category] * target_samples)\n",
    "\n",
    "        elif current_samples > target_samples:\n",
    "            # Under-sampling이 필요한 경우\n",
    "            indices = np.random.choice(current_samples, target_samples, replace=False)\n",
    "            processed_features.append(category_features[indices])\n",
    "            processed_categories.extend([category] * target_samples)\n",
    "\n",
    "        else:\n",
    "            # 현재 샘플 수가 목표와 동일한 경우\n",
    "            processed_features.append(category_features)\n",
    "            processed_categories.extend([category] * current_samples)\n",
    "\n",
    "    # 모든 처리된 데이터 합치기\n",
    "    X_combined = np.vstack(processed_features)\n",
    "    categories_combined = np.array(processed_categories)\n",
    "\n",
    "    # 특성 스케일링\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    num_categories = 5\n",
    "    y_encoded = np.eye(num_categories)[categories_combined]\n",
    "\n",
    "    print(\"Final category counts:\", np.bincount(categories_combined))\n",
    "\n",
    "    return X_scaled, y_encoded, num_categories"
   ],
   "id": "ec818e3d214c56b5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:56:19.181620Z",
     "start_time": "2024-11-26T06:56:19.168664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                num_epochs=100, patience=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)  # Classification 문제로 바뀌었기 때문에 이 부분 수정. 기존에는 y_batch(-1,1)여서 연속적인 값 예측시 사용하는 것\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, actual = torch.max(y_batch.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "id": "a481aa6b9ffbe221",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:56:25.971959Z",
     "start_time": "2024-11-26T06:56:19.197568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    df = pd.read_csv('spotify_dataset.csv')\n",
    "    X_scaled, y_encoded, num_categories = preprocess_data_with_augmentation(df)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = SpotifyDataset(X_train, y_train)\n",
    "    val_dataset = SpotifyDataset(X_val, y_val)\n",
    "    test_dataset = SpotifyDataset(X_test, y_test)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SpotifyRankPredictor(num_categories)\n",
    "    criterion = nn.CrossEntropyLoss()  # 분류 문제여서 기존 MSE 에서 CrossEntropy로 바꿈\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            y_true.extend(actual.numpy())\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "523e2a81871cdeda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original category counts: [253 325 235 565 812]\n",
      "Target samples per category: 649\n",
      "Final category counts: [649 649 649 649 649]\n",
      "Epoch [1/100], Train Loss: 1.5916, Val Loss: 1.5756, Val Accuracy: 28.70%\n",
      "Epoch [2/100], Train Loss: 1.5653, Val Loss: 1.5681, Val Accuracy: 30.25%\n",
      "Epoch [3/100], Train Loss: 1.5550, Val Loss: 1.5603, Val Accuracy: 29.94%\n",
      "Epoch [4/100], Train Loss: 1.5458, Val Loss: 1.5577, Val Accuracy: 31.79%\n",
      "Epoch [5/100], Train Loss: 1.5396, Val Loss: 1.5555, Val Accuracy: 31.48%\n",
      "Epoch [6/100], Train Loss: 1.5354, Val Loss: 1.5505, Val Accuracy: 31.79%\n",
      "Epoch [7/100], Train Loss: 1.5326, Val Loss: 1.5507, Val Accuracy: 32.72%\n",
      "Epoch [8/100], Train Loss: 1.5295, Val Loss: 1.5486, Val Accuracy: 33.02%\n",
      "Epoch [9/100], Train Loss: 1.5286, Val Loss: 1.5467, Val Accuracy: 34.57%\n",
      "Epoch [10/100], Train Loss: 1.5266, Val Loss: 1.5447, Val Accuracy: 33.95%\n",
      "Epoch [11/100], Train Loss: 1.5284, Val Loss: 1.5451, Val Accuracy: 34.88%\n",
      "Epoch [12/100], Train Loss: 1.5253, Val Loss: 1.5465, Val Accuracy: 32.72%\n",
      "Epoch [13/100], Train Loss: 1.5215, Val Loss: 1.5437, Val Accuracy: 33.33%\n",
      "Epoch [14/100], Train Loss: 1.5195, Val Loss: 1.5434, Val Accuracy: 32.72%\n",
      "Epoch [15/100], Train Loss: 1.5191, Val Loss: 1.5416, Val Accuracy: 33.33%\n",
      "Epoch [16/100], Train Loss: 1.5221, Val Loss: 1.5413, Val Accuracy: 33.64%\n",
      "Epoch [17/100], Train Loss: 1.5218, Val Loss: 1.5414, Val Accuracy: 32.72%\n",
      "Epoch [18/100], Train Loss: 1.5151, Val Loss: 1.5437, Val Accuracy: 33.33%\n",
      "Epoch [19/100], Train Loss: 1.5185, Val Loss: 1.5408, Val Accuracy: 32.72%\n",
      "Epoch [20/100], Train Loss: 1.5150, Val Loss: 1.5422, Val Accuracy: 31.48%\n",
      "Epoch [21/100], Train Loss: 1.5147, Val Loss: 1.5400, Val Accuracy: 32.10%\n",
      "Epoch [22/100], Train Loss: 1.5141, Val Loss: 1.5384, Val Accuracy: 34.88%\n",
      "Epoch [23/100], Train Loss: 1.5156, Val Loss: 1.5391, Val Accuracy: 32.72%\n",
      "Epoch [24/100], Train Loss: 1.5142, Val Loss: 1.5386, Val Accuracy: 33.64%\n",
      "Epoch [25/100], Train Loss: 1.5113, Val Loss: 1.5404, Val Accuracy: 32.72%\n",
      "Epoch [26/100], Train Loss: 1.5140, Val Loss: 1.5384, Val Accuracy: 33.02%\n",
      "Epoch [27/100], Train Loss: 1.5094, Val Loss: 1.5358, Val Accuracy: 33.95%\n",
      "Epoch [28/100], Train Loss: 1.5139, Val Loss: 1.5336, Val Accuracy: 34.26%\n",
      "Epoch [29/100], Train Loss: 1.5118, Val Loss: 1.5350, Val Accuracy: 33.64%\n",
      "Epoch [30/100], Train Loss: 1.5108, Val Loss: 1.5354, Val Accuracy: 33.33%\n",
      "Epoch [31/100], Train Loss: 1.5077, Val Loss: 1.5341, Val Accuracy: 33.95%\n",
      "Epoch [32/100], Train Loss: 1.5102, Val Loss: 1.5355, Val Accuracy: 34.26%\n",
      "Epoch [33/100], Train Loss: 1.5116, Val Loss: 1.5361, Val Accuracy: 32.72%\n",
      "Epoch [34/100], Train Loss: 1.5081, Val Loss: 1.5334, Val Accuracy: 33.64%\n",
      "Epoch [35/100], Train Loss: 1.5112, Val Loss: 1.5330, Val Accuracy: 34.26%\n",
      "Epoch [36/100], Train Loss: 1.5104, Val Loss: 1.5318, Val Accuracy: 33.95%\n",
      "Epoch [37/100], Train Loss: 1.5108, Val Loss: 1.5317, Val Accuracy: 35.19%\n",
      "Epoch [38/100], Train Loss: 1.5112, Val Loss: 1.5327, Val Accuracy: 35.49%\n",
      "Epoch [39/100], Train Loss: 1.5078, Val Loss: 1.5316, Val Accuracy: 34.26%\n",
      "Epoch [40/100], Train Loss: 1.5077, Val Loss: 1.5330, Val Accuracy: 34.57%\n",
      "Epoch [41/100], Train Loss: 1.5047, Val Loss: 1.5327, Val Accuracy: 33.02%\n",
      "Epoch [42/100], Train Loss: 1.5072, Val Loss: 1.5330, Val Accuracy: 35.19%\n",
      "Epoch [43/100], Train Loss: 1.5065, Val Loss: 1.5330, Val Accuracy: 33.95%\n",
      "Epoch [44/100], Train Loss: 1.5086, Val Loss: 1.5313, Val Accuracy: 33.64%\n",
      "Epoch [45/100], Train Loss: 1.5048, Val Loss: 1.5361, Val Accuracy: 31.48%\n",
      "Epoch [46/100], Train Loss: 1.5036, Val Loss: 1.5312, Val Accuracy: 33.64%\n",
      "Epoch [47/100], Train Loss: 1.5003, Val Loss: 1.5322, Val Accuracy: 33.02%\n",
      "Epoch [48/100], Train Loss: 1.5032, Val Loss: 1.5332, Val Accuracy: 33.33%\n",
      "Epoch [49/100], Train Loss: 1.5062, Val Loss: 1.5350, Val Accuracy: 32.41%\n",
      "Epoch [50/100], Train Loss: 1.5056, Val Loss: 1.5299, Val Accuracy: 33.64%\n",
      "Epoch [51/100], Train Loss: 1.5063, Val Loss: 1.5329, Val Accuracy: 33.33%\n",
      "Epoch [52/100], Train Loss: 1.5029, Val Loss: 1.5307, Val Accuracy: 33.64%\n",
      "Epoch [53/100], Train Loss: 1.5027, Val Loss: 1.5298, Val Accuracy: 34.26%\n",
      "Epoch [54/100], Train Loss: 1.5075, Val Loss: 1.5296, Val Accuracy: 33.95%\n",
      "Epoch [55/100], Train Loss: 1.5010, Val Loss: 1.5342, Val Accuracy: 32.41%\n",
      "Epoch [56/100], Train Loss: 1.5017, Val Loss: 1.5332, Val Accuracy: 32.41%\n",
      "Epoch [57/100], Train Loss: 1.5058, Val Loss: 1.5307, Val Accuracy: 32.41%\n",
      "Epoch [58/100], Train Loss: 1.5041, Val Loss: 1.5295, Val Accuracy: 33.64%\n",
      "Epoch [59/100], Train Loss: 1.5044, Val Loss: 1.5339, Val Accuracy: 32.41%\n",
      "Epoch [60/100], Train Loss: 1.5031, Val Loss: 1.5279, Val Accuracy: 34.88%\n",
      "Epoch [61/100], Train Loss: 1.5043, Val Loss: 1.5277, Val Accuracy: 33.95%\n",
      "Epoch [62/100], Train Loss: 1.5048, Val Loss: 1.5287, Val Accuracy: 34.88%\n",
      "Epoch [63/100], Train Loss: 1.4980, Val Loss: 1.5312, Val Accuracy: 31.79%\n",
      "Epoch [64/100], Train Loss: 1.4981, Val Loss: 1.5250, Val Accuracy: 34.57%\n",
      "Epoch [65/100], Train Loss: 1.5019, Val Loss: 1.5287, Val Accuracy: 32.72%\n",
      "Epoch [66/100], Train Loss: 1.5050, Val Loss: 1.5279, Val Accuracy: 36.42%\n",
      "Epoch [67/100], Train Loss: 1.5012, Val Loss: 1.5271, Val Accuracy: 33.95%\n",
      "Epoch [68/100], Train Loss: 1.5022, Val Loss: 1.5266, Val Accuracy: 34.88%\n",
      "Epoch [69/100], Train Loss: 1.5036, Val Loss: 1.5252, Val Accuracy: 34.57%\n",
      "Epoch [70/100], Train Loss: 1.5048, Val Loss: 1.5290, Val Accuracy: 33.02%\n",
      "Epoch [71/100], Train Loss: 1.4951, Val Loss: 1.5315, Val Accuracy: 32.10%\n",
      "Epoch [72/100], Train Loss: 1.5041, Val Loss: 1.5281, Val Accuracy: 33.33%\n",
      "Epoch [73/100], Train Loss: 1.5048, Val Loss: 1.5279, Val Accuracy: 32.72%\n",
      "Epoch [74/100], Train Loss: 1.4939, Val Loss: 1.5289, Val Accuracy: 31.79%\n",
      "Early stopping triggered\n",
      "Test Accuracy: 30.15%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28  9 13  7 21]\n",
      " [15  7 16 13 23]\n",
      " [17 12 11  6 19]\n",
      " [ 2  2  2 16 32]\n",
      " [ 0  2  0 16 36]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이강민\\AppData\\Local\\Temp\\ipykernel_17188\\3380637125.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
