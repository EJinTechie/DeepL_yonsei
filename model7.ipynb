{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T02:32:15.454313Z",
     "start_time": "2024-12-05T02:32:08.601647Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:32:15.484836Z",
     "start_time": "2024-12-05T02:32:15.469336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpotifyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SpotifyRankPredictor(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super(SpotifyRankPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, num_categories)  # 출력층이 1개에서 5개(카테고리 수)로 변경\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax 활성화 함수 추가하여 각 카테고리의 확률 출력\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.softmax(x)"
   ],
   "id": "bfeb3bf9ec46e9be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:32:15.499786Z",
     "start_time": "2024-12-05T02:32:15.489820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_features(features, category, num_augmentations):\n",
    "    augmented_data = []\n",
    "    feature_names = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                     'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']\n",
    "\n",
    "    noise_ranges = {\n",
    "        'Danceability': 0.05,\n",
    "        'Energy': 0.05,\n",
    "        'Loudness': 1.0,\n",
    "        'Speechiness': 0.02,\n",
    "        'Acousticness': 0.05,\n",
    "        'Liveness': 0.05,\n",
    "        'Tempo': 3.0,\n",
    "        'Duration (ms)': 0.05  # 5% 변화\n",
    "    }\n",
    "    # features의 각 행에 대해 증강을 수행\n",
    "    for feature_row in features:\n",
    "        for _ in range(num_augmentations):\n",
    "            new_features = []\n",
    "            for feat_idx, feat_name in enumerate(feature_names):\n",
    "                feature = np.abs(feature_row[feat_idx])  # 개별 행의 특성값 사용\n",
    "\n",
    "                # 나머지 로직은 동일...\n",
    "                if feat_name == 'Loudness':\n",
    "                    noise = np.random.normal(0, np.abs(noise_ranges['Loudness']))\n",
    "                elif feat_name == 'Tempo':\n",
    "                    noise = np.random.normal(0, np.abs(noise_ranges['Tempo']))\n",
    "                elif feat_name == 'Duration (ms)':\n",
    "                    noise = np.random.normal(0, np.abs(feature * noise_ranges['Duration (ms)']))\n",
    "                else:\n",
    "                    noise = np.random.normal(0, np.abs(noise_ranges[feat_name]))\n",
    "\n",
    "                new_value = feature + noise\n",
    "\n",
    "                if feat_name == 'Loudness':\n",
    "                    new_value = np.clip(new_value, -60, 0)\n",
    "                elif feat_name == 'Duration (ms)':\n",
    "                    new_value = max(1000, new_value)\n",
    "                else:\n",
    "                    new_value = np.clip(new_value, 0, 1)\n",
    "\n",
    "                new_features.append(new_value)\n",
    "\n",
    "            augmented_data.append(new_features)\n",
    "\n",
    "    return np.array(augmented_data)"
   ],
   "id": "2aa7dd05d3541c3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:32:15.514736Z",
     "start_time": "2024-12-05T02:32:15.504771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(df):\n",
    "    X = df[['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "            'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']].values\n",
    "\n",
    "    # 특성 스케일링\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    def rank_to_category(rank):\n",
    "        if rank <= 10:\n",
    "            return 0\n",
    "        elif rank <= 30:\n",
    "            return 1\n",
    "        elif rank <= 50:\n",
    "            return 2\n",
    "        elif rank <= 100:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    ranks = df['Highest Charting Position'].values\n",
    "    categories = np.array([rank_to_category(rank) for rank in ranks])\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    num_categories = 5\n",
    "    y_encoded = np.eye(num_categories)[categories]\n",
    "\n",
    "    return X_scaled, y_encoded, num_categories\n",
    "\n",
    "def augment_data(X, y):\n",
    "    # one-hot 인코딩된 y를 다시 카테고리로 변환\n",
    "    categories = np.argmax(y, axis=1)\n",
    "\n",
    "    augmentation_ratios = {\n",
    "        0: 5,  # Top 10\n",
    "        1: 4,  # Top 11-30\n",
    "        2: 3,  # Top 31-50\n",
    "        3: 2,  # Top 51-100\n",
    "        4: 0   # Below 100\n",
    "    }\n",
    "\n",
    "    augmented_features = []\n",
    "    augmented_categories = []\n",
    "\n",
    "    for category in range(5):\n",
    "        category_mask = categories == category\n",
    "        category_features = X[category_mask]\n",
    "\n",
    "        if category != 4:  # Below 100이 아닌 경우에만 증강\n",
    "            new_features = augment_features(\n",
    "                category_features,\n",
    "                category,\n",
    "                augmentation_ratios[category]\n",
    "            )\n",
    "            augmented_features.append(new_features)\n",
    "            augmented_categories.extend([category] * len(new_features))\n",
    "\n",
    "    # 원본 데이터와 증강 데이터 합치기\n",
    "    if augmented_features:\n",
    "        augmented_features = np.vstack(augmented_features)\n",
    "        X_combined = np.vstack([X, augmented_features])\n",
    "        # 증강된 카테고리를 다시 원-핫 인코딩\n",
    "        augmented_categories = np.eye(5)[augmented_categories]\n",
    "        y_combined = np.vstack([y, augmented_categories])\n",
    "    else:\n",
    "        X_combined = X\n",
    "        y_combined = y\n",
    "\n",
    "    return X_combined, y_combined\n"
   ],
   "id": "ec818e3d214c56b5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:32:15.814092Z",
     "start_time": "2024-12-05T02:32:15.800139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                num_epochs=100, patience=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)  # Classification 문제로 바뀌었기 때문에 이 부분 수정. 기존에는 y_batch(-1,1)여서 연속적인 값 예측시 사용하는 것\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, actual = torch.max(y_batch.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "id": "a481aa6b9ffbe221",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:32:26.776341Z",
     "start_time": "2024-12-05T02:32:15.856948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # 1. 데이터 로드 및 전처리\n",
    "    df = pd.read_csv('spotify_dataset.csv')\n",
    "    X_scaled, y_encoded, num_categories = preprocess_data(df)\n",
    "\n",
    "    # 2. 먼저 train/temp 분할 (증강 전)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3. temp를 validation과 test로 분할\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    # 4. train 데이터에만 증강 적용\n",
    "    X_train_augmented, y_train_augmented = augment_data(X_train, y_train)\n",
    "\n",
    "    # 5. 데이터셋 생성\n",
    "    train_dataset = SpotifyDataset(X_train_augmented, y_train_augmented)\n",
    "    val_dataset = SpotifyDataset(X_val, y_val)\n",
    "    test_dataset = SpotifyDataset(X_test, y_test)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SpotifyRankPredictor(num_categories)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Confusion Matrix 계산\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            y_true.extend(actual.numpy())\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "523e2a81871cdeda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.5666, Val Loss: 1.4899, Val Accuracy: 42.92%\n",
      "Epoch [2/100], Train Loss: 1.5536, Val Loss: 1.4812, Val Accuracy: 42.92%\n",
      "Epoch [3/100], Train Loss: 1.5471, Val Loss: 1.4770, Val Accuracy: 42.92%\n",
      "Epoch [4/100], Train Loss: 1.5436, Val Loss: 1.4748, Val Accuracy: 42.47%\n",
      "Epoch [5/100], Train Loss: 1.5418, Val Loss: 1.4736, Val Accuracy: 42.92%\n",
      "Epoch [6/100], Train Loss: 1.5418, Val Loss: 1.4717, Val Accuracy: 42.92%\n",
      "Epoch [7/100], Train Loss: 1.5410, Val Loss: 1.4762, Val Accuracy: 42.47%\n",
      "Epoch [8/100], Train Loss: 1.5404, Val Loss: 1.4776, Val Accuracy: 42.01%\n",
      "Epoch [9/100], Train Loss: 1.5405, Val Loss: 1.4727, Val Accuracy: 42.92%\n",
      "Epoch [10/100], Train Loss: 1.5380, Val Loss: 1.4777, Val Accuracy: 42.92%\n",
      "Epoch [11/100], Train Loss: 1.5390, Val Loss: 1.4750, Val Accuracy: 42.92%\n",
      "Epoch [12/100], Train Loss: 1.5405, Val Loss: 1.4763, Val Accuracy: 43.38%\n",
      "Epoch [13/100], Train Loss: 1.5387, Val Loss: 1.4731, Val Accuracy: 42.92%\n",
      "Epoch [14/100], Train Loss: 1.5398, Val Loss: 1.4801, Val Accuracy: 42.92%\n",
      "Epoch [15/100], Train Loss: 1.5399, Val Loss: 1.4733, Val Accuracy: 42.92%\n",
      "Epoch [16/100], Train Loss: 1.5388, Val Loss: 1.4703, Val Accuracy: 42.92%\n",
      "Epoch [17/100], Train Loss: 1.5386, Val Loss: 1.4788, Val Accuracy: 42.01%\n",
      "Epoch [18/100], Train Loss: 1.5375, Val Loss: 1.4756, Val Accuracy: 42.92%\n",
      "Epoch [19/100], Train Loss: 1.5382, Val Loss: 1.4862, Val Accuracy: 41.55%\n",
      "Epoch [20/100], Train Loss: 1.5367, Val Loss: 1.4837, Val Accuracy: 41.55%\n",
      "Epoch [21/100], Train Loss: 1.5385, Val Loss: 1.4776, Val Accuracy: 42.01%\n",
      "Epoch [22/100], Train Loss: 1.5381, Val Loss: 1.4768, Val Accuracy: 42.92%\n",
      "Epoch [23/100], Train Loss: 1.5374, Val Loss: 1.4739, Val Accuracy: 42.92%\n",
      "Epoch [24/100], Train Loss: 1.5364, Val Loss: 1.4690, Val Accuracy: 42.47%\n",
      "Epoch [25/100], Train Loss: 1.5357, Val Loss: 1.4700, Val Accuracy: 42.92%\n",
      "Epoch [26/100], Train Loss: 1.5347, Val Loss: 1.4698, Val Accuracy: 43.38%\n",
      "Epoch [27/100], Train Loss: 1.5363, Val Loss: 1.4731, Val Accuracy: 43.84%\n",
      "Epoch [28/100], Train Loss: 1.5352, Val Loss: 1.4686, Val Accuracy: 43.38%\n",
      "Epoch [29/100], Train Loss: 1.5347, Val Loss: 1.4760, Val Accuracy: 43.38%\n",
      "Epoch [30/100], Train Loss: 1.5361, Val Loss: 1.4687, Val Accuracy: 44.29%\n",
      "Epoch [31/100], Train Loss: 1.5346, Val Loss: 1.4739, Val Accuracy: 43.38%\n",
      "Epoch [32/100], Train Loss: 1.5360, Val Loss: 1.4749, Val Accuracy: 42.92%\n",
      "Epoch [33/100], Train Loss: 1.5350, Val Loss: 1.4755, Val Accuracy: 42.92%\n",
      "Epoch [34/100], Train Loss: 1.5330, Val Loss: 1.4741, Val Accuracy: 43.38%\n",
      "Epoch [35/100], Train Loss: 1.5336, Val Loss: 1.4731, Val Accuracy: 43.38%\n",
      "Epoch [36/100], Train Loss: 1.5343, Val Loss: 1.4677, Val Accuracy: 43.38%\n",
      "Epoch [37/100], Train Loss: 1.5335, Val Loss: 1.4681, Val Accuracy: 43.38%\n",
      "Epoch [38/100], Train Loss: 1.5339, Val Loss: 1.4772, Val Accuracy: 42.92%\n",
      "Epoch [39/100], Train Loss: 1.5351, Val Loss: 1.4718, Val Accuracy: 43.38%\n",
      "Epoch [40/100], Train Loss: 1.5348, Val Loss: 1.4805, Val Accuracy: 43.38%\n",
      "Epoch [41/100], Train Loss: 1.5338, Val Loss: 1.4775, Val Accuracy: 42.92%\n",
      "Epoch [42/100], Train Loss: 1.5339, Val Loss: 1.4721, Val Accuracy: 42.92%\n",
      "Epoch [43/100], Train Loss: 1.5339, Val Loss: 1.4760, Val Accuracy: 42.92%\n",
      "Epoch [44/100], Train Loss: 1.5321, Val Loss: 1.4752, Val Accuracy: 43.38%\n",
      "Epoch [45/100], Train Loss: 1.5341, Val Loss: 1.4629, Val Accuracy: 43.84%\n",
      "Epoch [46/100], Train Loss: 1.5327, Val Loss: 1.4673, Val Accuracy: 44.29%\n",
      "Epoch [47/100], Train Loss: 1.5346, Val Loss: 1.4699, Val Accuracy: 43.38%\n",
      "Epoch [48/100], Train Loss: 1.5320, Val Loss: 1.4667, Val Accuracy: 43.84%\n",
      "Epoch [49/100], Train Loss: 1.5336, Val Loss: 1.4726, Val Accuracy: 44.29%\n",
      "Epoch [50/100], Train Loss: 1.5350, Val Loss: 1.4755, Val Accuracy: 42.92%\n",
      "Epoch [51/100], Train Loss: 1.5334, Val Loss: 1.4704, Val Accuracy: 42.92%\n",
      "Epoch [52/100], Train Loss: 1.5329, Val Loss: 1.4729, Val Accuracy: 41.55%\n",
      "Epoch [53/100], Train Loss: 1.5324, Val Loss: 1.4718, Val Accuracy: 42.01%\n",
      "Epoch [54/100], Train Loss: 1.5315, Val Loss: 1.4710, Val Accuracy: 42.47%\n",
      "Epoch [55/100], Train Loss: 1.5321, Val Loss: 1.4825, Val Accuracy: 42.47%\n",
      "Early stopping triggered\n",
      "Test Accuracy: 42.47%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0  3 18]\n",
      " [ 0  1  0  2 28]\n",
      " [ 0  0  0  2 25]\n",
      " [ 0  0  0  7 44]\n",
      " [ 0  3  0  1 85]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이강민\\AppData\\Local\\Temp\\ipykernel_3292\\2573774415.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
