{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-26T04:55:33.849823Z",
     "start_time": "2024-11-26T04:55:33.842847Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T04:55:33.879632Z",
     "start_time": "2024-11-26T04:55:33.859789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpotifyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SpotifyRankPredictor(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super(SpotifyRankPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, num_categories)  # 출력층이 1개에서 5개(카테고리 수)로 변경\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax 활성화 함수 추가하여 각 카테고리의 확률 출력\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.softmax(x)"
   ],
   "id": "bfeb3bf9ec46e9be",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T04:55:33.909565Z",
     "start_time": "2024-11-26T04:55:33.896609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_features(features, category, num_augmentations):\n",
    "    augmented_data = []\n",
    "    feature_names = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                     'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']\n",
    "\n",
    "    noise_ranges = {\n",
    "        'Danceability': 0.05,\n",
    "        'Energy': 0.05,\n",
    "        'Loudness': 1.0,\n",
    "        'Speechiness': 0.02,\n",
    "        'Acousticness': 0.05,\n",
    "        'Liveness': 0.05,\n",
    "        'Tempo': 3.0,\n",
    "        'Duration (ms)': 0.05  # 5% 변화\n",
    "    }\n",
    "    # features의 각 행에 대해 증강을 수행\n",
    "    for feature_row in features:\n",
    "        for _ in range(num_augmentations):\n",
    "            new_features = []\n",
    "            for feat_idx, feat_name in enumerate(feature_names):\n",
    "                feature = np.abs(feature_row[feat_idx])  # 개별 행의 특성값 사용\n",
    "\n",
    "                # 나머지 로직은 동일...\n",
    "                if feat_name == 'Loudness':\n",
    "                    noise = np.random.normal(0, np.abs(noise_ranges['Loudness']))\n",
    "                elif feat_name == 'Tempo':\n",
    "                    noise = np.random.normal(0, np.abs(noise_ranges['Tempo']))\n",
    "                elif feat_name == 'Duration (ms)':\n",
    "                    noise = np.random.normal(0, np.abs(feature * noise_ranges['Duration (ms)']))\n",
    "                else:\n",
    "                    noise = np.random.normal(0, np.abs(noise_ranges[feat_name]))\n",
    "\n",
    "                new_value = feature + noise\n",
    "\n",
    "                if feat_name == 'Loudness':\n",
    "                    new_value = np.clip(new_value, -60, 0)\n",
    "                elif feat_name == 'Duration (ms)':\n",
    "                    new_value = max(1000, new_value)\n",
    "                else:\n",
    "                    new_value = np.clip(new_value, 0, 1)\n",
    "\n",
    "                new_features.append(new_value)\n",
    "\n",
    "            augmented_data.append(new_features)\n",
    "\n",
    "    return np.array(augmented_data)"
   ],
   "id": "2aa7dd05d3541c3",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T04:55:33.939420Z",
     "start_time": "2024-11-26T04:55:33.926276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data_with_augmentation(df):\n",
    "\n",
    "    X = df[['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "            'Acousticness', 'Liveness', 'Tempo', 'Duration (ms)']].values\n",
    "\n",
    "    def rank_to_category(rank):\n",
    "        if rank <= 10:\n",
    "            return 0\n",
    "        elif rank <= 30:\n",
    "            return 1\n",
    "        elif rank <= 50:\n",
    "            return 2\n",
    "        elif rank <= 100:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    ranks = df['Highest Charting Position'].values\n",
    "    categories = np.array([rank_to_category(rank) for rank in ranks])\n",
    "\n",
    "    # 각 카테고리별 증강 비율 설정\n",
    "    augmentation_ratios = {\n",
    "        0: 5,  # Top 10\n",
    "        1: 4,  # Top 11-30\n",
    "        2: 3,  # Top 31-50\n",
    "        3: 2,  # Top 51-100\n",
    "        4: 0   # Below 100\n",
    "    }\n",
    "\n",
    "\n",
    "    augmented_features = []\n",
    "    augmented_categories = []\n",
    "\n",
    "    for category in range(5):\n",
    "        category_mask = categories == category\n",
    "        category_features = X[category_mask]\n",
    "\n",
    "        if category != 4:  # Below 100이 아닌 경우에만 증강\n",
    "            # 증강 데이터 생성\n",
    "            new_features = augment_features(\n",
    "                category_features,\n",
    "                category,\n",
    "                len(category_features) * augmentation_ratios[category]\n",
    "            )\n",
    "\n",
    "            augmented_features.append(new_features)\n",
    "            augmented_categories.extend([category] * len(new_features))\n",
    "\n",
    "    # 원본 데이터와 증강 데이터 합치기\n",
    "    if augmented_features:\n",
    "        augmented_features = np.vstack(augmented_features)\n",
    "        X_combined = np.vstack([X, augmented_features])\n",
    "        categories_combined = np.concatenate([categories, augmented_categories])\n",
    "    else:\n",
    "        X_combined = X\n",
    "        categories_combined = categories\n",
    "\n",
    "    # 특성 스케일링\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "    # 원-핫 인코딩\n",
    "    num_categories = 5\n",
    "    y_encoded = np.eye(num_categories)[categories_combined]\n",
    "\n",
    "    return X_scaled, y_encoded, num_categories"
   ],
   "id": "ec818e3d214c56b5",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T04:55:33.969083Z",
     "start_time": "2024-11-26T04:55:33.956257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                num_epochs=100, patience=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)  # Classification 문제로 바뀌었기 때문에 이 부분 수정. 기존에는 y_batch(-1,1)여서 연속적인 값 예측시 사용하는 것\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, actual = torch.max(y_batch.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "id": "a481aa6b9ffbe221",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:01:06.277849Z",
     "start_time": "2024-11-26T04:55:33.986706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    df = pd.read_csv('spotify_dataset.csv')\n",
    "    X_scaled, y_encoded, num_categories = preprocess_data_with_augmentation(df)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = SpotifyDataset(X_train, y_train)\n",
    "    val_dataset = SpotifyDataset(X_val, y_val)\n",
    "    test_dataset = SpotifyDataset(X_test, y_test)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SpotifyRankPredictor(num_categories)\n",
    "    criterion = nn.CrossEntropyLoss()  # 분류 문제여서 기존 MSE 에서 CrossEntropy로 바꿈\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, actual = torch.max(y_batch.data, 1)\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            y_true.extend(actual.numpy())\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "523e2a81871cdeda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.4426, Val Loss: 1.4151, Val Accuracy: 47.88%\n",
      "Epoch [2/100], Train Loss: 1.4272, Val Loss: 1.4068, Val Accuracy: 48.56%\n",
      "Epoch [3/100], Train Loss: 1.4238, Val Loss: 1.3998, Val Accuracy: 49.37%\n",
      "Epoch [4/100], Train Loss: 1.4192, Val Loss: 1.3967, Val Accuracy: 49.86%\n",
      "Epoch [5/100], Train Loss: 1.4182, Val Loss: 1.3981, Val Accuracy: 49.69%\n",
      "Epoch [6/100], Train Loss: 1.4191, Val Loss: 1.3941, Val Accuracy: 50.20%\n",
      "Epoch [7/100], Train Loss: 1.4178, Val Loss: 1.3927, Val Accuracy: 50.14%\n",
      "Epoch [8/100], Train Loss: 1.4172, Val Loss: 1.3953, Val Accuracy: 49.81%\n",
      "Epoch [9/100], Train Loss: 1.4161, Val Loss: 1.3929, Val Accuracy: 50.11%\n",
      "Epoch [10/100], Train Loss: 1.4159, Val Loss: 1.3946, Val Accuracy: 50.00%\n",
      "Epoch [11/100], Train Loss: 1.4161, Val Loss: 1.3944, Val Accuracy: 50.02%\n",
      "Epoch [12/100], Train Loss: 1.4152, Val Loss: 1.3903, Val Accuracy: 50.36%\n",
      "Epoch [13/100], Train Loss: 1.4149, Val Loss: 1.3921, Val Accuracy: 50.29%\n",
      "Epoch [14/100], Train Loss: 1.4149, Val Loss: 1.3894, Val Accuracy: 50.57%\n",
      "Epoch [15/100], Train Loss: 1.4143, Val Loss: 1.3886, Val Accuracy: 50.59%\n",
      "Epoch [16/100], Train Loss: 1.4139, Val Loss: 1.3875, Val Accuracy: 51.00%\n",
      "Epoch [17/100], Train Loss: 1.4134, Val Loss: 1.3887, Val Accuracy: 50.72%\n",
      "Epoch [18/100], Train Loss: 1.4131, Val Loss: 1.3863, Val Accuracy: 50.91%\n",
      "Epoch [19/100], Train Loss: 1.4127, Val Loss: 1.3850, Val Accuracy: 51.03%\n",
      "Epoch [20/100], Train Loss: 1.4116, Val Loss: 1.3870, Val Accuracy: 50.89%\n",
      "Epoch [21/100], Train Loss: 1.4112, Val Loss: 1.3826, Val Accuracy: 51.23%\n",
      "Epoch [22/100], Train Loss: 1.4112, Val Loss: 1.3838, Val Accuracy: 51.16%\n",
      "Epoch [23/100], Train Loss: 1.4105, Val Loss: 1.3808, Val Accuracy: 51.47%\n",
      "Epoch [24/100], Train Loss: 1.4100, Val Loss: 1.3820, Val Accuracy: 51.25%\n",
      "Epoch [25/100], Train Loss: 1.4101, Val Loss: 1.3843, Val Accuracy: 51.18%\n",
      "Epoch [26/100], Train Loss: 1.4097, Val Loss: 1.3829, Val Accuracy: 51.23%\n",
      "Epoch [27/100], Train Loss: 1.4096, Val Loss: 1.3885, Val Accuracy: 50.61%\n",
      "Epoch [28/100], Train Loss: 1.4092, Val Loss: 1.3790, Val Accuracy: 51.52%\n",
      "Epoch [29/100], Train Loss: 1.4091, Val Loss: 1.3812, Val Accuracy: 51.46%\n",
      "Epoch [30/100], Train Loss: 1.4087, Val Loss: 1.3802, Val Accuracy: 51.42%\n",
      "Epoch [31/100], Train Loss: 1.4089, Val Loss: 1.3803, Val Accuracy: 51.50%\n",
      "Epoch [32/100], Train Loss: 1.4087, Val Loss: 1.3794, Val Accuracy: 51.72%\n",
      "Epoch [33/100], Train Loss: 1.4082, Val Loss: 1.3778, Val Accuracy: 51.87%\n",
      "Epoch [34/100], Train Loss: 1.4085, Val Loss: 1.3817, Val Accuracy: 51.50%\n",
      "Epoch [35/100], Train Loss: 1.4083, Val Loss: 1.3770, Val Accuracy: 51.92%\n",
      "Epoch [36/100], Train Loss: 1.4079, Val Loss: 1.3813, Val Accuracy: 51.48%\n",
      "Epoch [37/100], Train Loss: 1.4074, Val Loss: 1.3793, Val Accuracy: 51.76%\n",
      "Epoch [38/100], Train Loss: 1.4077, Val Loss: 1.3798, Val Accuracy: 51.60%\n",
      "Epoch [39/100], Train Loss: 1.4082, Val Loss: 1.3805, Val Accuracy: 51.48%\n",
      "Epoch [40/100], Train Loss: 1.4078, Val Loss: 1.3770, Val Accuracy: 51.99%\n",
      "Epoch [41/100], Train Loss: 1.4078, Val Loss: 1.3817, Val Accuracy: 51.40%\n",
      "Epoch [42/100], Train Loss: 1.4077, Val Loss: 1.3794, Val Accuracy: 51.67%\n",
      "Epoch [43/100], Train Loss: 1.4079, Val Loss: 1.3760, Val Accuracy: 52.02%\n",
      "Epoch [44/100], Train Loss: 1.4073, Val Loss: 1.3795, Val Accuracy: 51.74%\n",
      "Epoch [45/100], Train Loss: 1.4071, Val Loss: 1.3790, Val Accuracy: 51.86%\n",
      "Epoch [46/100], Train Loss: 1.4080, Val Loss: 1.3787, Val Accuracy: 51.88%\n",
      "Epoch [47/100], Train Loss: 1.4076, Val Loss: 1.3797, Val Accuracy: 51.61%\n",
      "Epoch [48/100], Train Loss: 1.4075, Val Loss: 1.3749, Val Accuracy: 52.20%\n",
      "Epoch [49/100], Train Loss: 1.4072, Val Loss: 1.3822, Val Accuracy: 51.31%\n",
      "Epoch [50/100], Train Loss: 1.4070, Val Loss: 1.3779, Val Accuracy: 51.88%\n",
      "Epoch [51/100], Train Loss: 1.4068, Val Loss: 1.3779, Val Accuracy: 51.81%\n",
      "Epoch [52/100], Train Loss: 1.4071, Val Loss: 1.3767, Val Accuracy: 51.99%\n",
      "Epoch [53/100], Train Loss: 1.4071, Val Loss: 1.3783, Val Accuracy: 51.77%\n",
      "Epoch [54/100], Train Loss: 1.4070, Val Loss: 1.3765, Val Accuracy: 52.03%\n",
      "Epoch [55/100], Train Loss: 1.4071, Val Loss: 1.3796, Val Accuracy: 51.63%\n",
      "Epoch [56/100], Train Loss: 1.4067, Val Loss: 1.3771, Val Accuracy: 51.93%\n",
      "Epoch [57/100], Train Loss: 1.4065, Val Loss: 1.3761, Val Accuracy: 52.12%\n",
      "Epoch [58/100], Train Loss: 1.4066, Val Loss: 1.3746, Val Accuracy: 52.18%\n",
      "Epoch [59/100], Train Loss: 1.4068, Val Loss: 1.3764, Val Accuracy: 52.00%\n",
      "Epoch [60/100], Train Loss: 1.4069, Val Loss: 1.3777, Val Accuracy: 51.89%\n",
      "Epoch [61/100], Train Loss: 1.4065, Val Loss: 1.3752, Val Accuracy: 52.13%\n",
      "Epoch [62/100], Train Loss: 1.4064, Val Loss: 1.3770, Val Accuracy: 51.97%\n",
      "Epoch [63/100], Train Loss: 1.4063, Val Loss: 1.3746, Val Accuracy: 52.28%\n",
      "Epoch [64/100], Train Loss: 1.4065, Val Loss: 1.3762, Val Accuracy: 51.97%\n",
      "Epoch [65/100], Train Loss: 1.4066, Val Loss: 1.3745, Val Accuracy: 52.17%\n",
      "Epoch [66/100], Train Loss: 1.4063, Val Loss: 1.3775, Val Accuracy: 51.89%\n",
      "Epoch [67/100], Train Loss: 1.4061, Val Loss: 1.3761, Val Accuracy: 52.10%\n",
      "Epoch [68/100], Train Loss: 1.4063, Val Loss: 1.3786, Val Accuracy: 51.85%\n",
      "Epoch [69/100], Train Loss: 1.4058, Val Loss: 1.3772, Val Accuracy: 51.88%\n",
      "Epoch [70/100], Train Loss: 1.4057, Val Loss: 1.3799, Val Accuracy: 51.71%\n",
      "Epoch [71/100], Train Loss: 1.4061, Val Loss: 1.3768, Val Accuracy: 52.03%\n",
      "Epoch [72/100], Train Loss: 1.4062, Val Loss: 1.3740, Val Accuracy: 52.37%\n",
      "Epoch [73/100], Train Loss: 1.4060, Val Loss: 1.3749, Val Accuracy: 52.30%\n",
      "Epoch [74/100], Train Loss: 1.4056, Val Loss: 1.3768, Val Accuracy: 51.95%\n",
      "Epoch [75/100], Train Loss: 1.4058, Val Loss: 1.3756, Val Accuracy: 52.19%\n",
      "Epoch [76/100], Train Loss: 1.4053, Val Loss: 1.3745, Val Accuracy: 52.38%\n",
      "Epoch [77/100], Train Loss: 1.4057, Val Loss: 1.3768, Val Accuracy: 52.01%\n",
      "Epoch [78/100], Train Loss: 1.4052, Val Loss: 1.3765, Val Accuracy: 52.14%\n",
      "Epoch [79/100], Train Loss: 1.4051, Val Loss: 1.3763, Val Accuracy: 52.02%\n",
      "Epoch [80/100], Train Loss: 1.4055, Val Loss: 1.3740, Val Accuracy: 52.36%\n",
      "Epoch [81/100], Train Loss: 1.4054, Val Loss: 1.3744, Val Accuracy: 52.20%\n",
      "Epoch [82/100], Train Loss: 1.4053, Val Loss: 1.3733, Val Accuracy: 52.60%\n",
      "Epoch [83/100], Train Loss: 1.4052, Val Loss: 1.3755, Val Accuracy: 52.10%\n",
      "Epoch [84/100], Train Loss: 1.4055, Val Loss: 1.3728, Val Accuracy: 52.46%\n",
      "Epoch [85/100], Train Loss: 1.4051, Val Loss: 1.3774, Val Accuracy: 51.94%\n",
      "Epoch [86/100], Train Loss: 1.4051, Val Loss: 1.3723, Val Accuracy: 52.60%\n",
      "Epoch [87/100], Train Loss: 1.4050, Val Loss: 1.3790, Val Accuracy: 51.58%\n",
      "Epoch [88/100], Train Loss: 1.4052, Val Loss: 1.3767, Val Accuracy: 52.16%\n",
      "Epoch [89/100], Train Loss: 1.4052, Val Loss: 1.3758, Val Accuracy: 52.01%\n",
      "Epoch [90/100], Train Loss: 1.4048, Val Loss: 1.3737, Val Accuracy: 52.43%\n",
      "Epoch [91/100], Train Loss: 1.4047, Val Loss: 1.3738, Val Accuracy: 52.35%\n",
      "Epoch [92/100], Train Loss: 1.4047, Val Loss: 1.3737, Val Accuracy: 52.29%\n",
      "Epoch [93/100], Train Loss: 1.4052, Val Loss: 1.3771, Val Accuracy: 51.81%\n",
      "Epoch [94/100], Train Loss: 1.4049, Val Loss: 1.3745, Val Accuracy: 52.13%\n",
      "Epoch [95/100], Train Loss: 1.4049, Val Loss: 1.3745, Val Accuracy: 52.24%\n",
      "Epoch [96/100], Train Loss: 1.4051, Val Loss: 1.3737, Val Accuracy: 52.23%\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이강민\\AppData\\Local\\Temp\\ipykernel_6480\\3380637125.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.60%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10157  5714     9 15919     0]\n",
      " [ 3501 18867    23 19787     0]\n",
      " [ 1986  3784   147 10693     0]\n",
      " [ 4379  7528    20 52299     0]\n",
      " [    2     9     0    62     0]]\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
